{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random \n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM,GRU,SimpleRNN, RNN, Input, Bidirectional\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight') \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category= FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - Pathlib \n",
    "# - TF functionnal API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general settings\n",
    "class CFG:\n",
    "    data_folder = \"./data\"\n",
    "    img_dim1 = 20\n",
    "    img_dim2 = 10\n",
    "    nepochs = 6\n",
    "    seed = 42\n",
    "    EPOCH = 300\n",
    "    bsize = 16\n",
    "    BATCH_SIZE = 1024\n",
    "\n",
    "    \n",
    "# adjust the parameters for displayed figures    \n",
    "plt.rcParams.update({'figure.figsize': (CFG.img_dim1,CFG.img_dim2)})   \n",
    "\n",
    "\n",
    "def seed_everything(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_rmse(x,y):\n",
    "    return(np.round( np.sqrt(mse(x,y)) ,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, look_back, look_ahead):\n",
    "    xdat, ydat = [], []\n",
    "    for i in range(len(df) - look_back -look_ahead):\n",
    "        xdat.append(df[i:i+ look_back ,0])\n",
    "        ydat.append(df[i+ look_back : i + look_back + look_ahead,0])\n",
    "    xdat, ydat = np.array(xdat), np.array(ydat).reshape(-1,look_ahead)\n",
    "    return xdat, ydat\n",
    "\n",
    "def prepare_split(xdat, ydat, cutoff = 5000, timesteps = 50):\n",
    "    xtrain, xvalid = xdat[:cutoff,:], xdat[cutoff:,]\n",
    "    ytrain, yvalid = ydat[:cutoff,:], ydat[cutoff:,]\n",
    "\n",
    "    # reshape into [batch size, time steps, dimensionality]\n",
    "    xtrain = xtrain.reshape(-1, timesteps, 1)\n",
    "    xvalid = xvalid.reshape(-1, timesteps, 1)\n",
    "\n",
    "    return xtrain, ytrain, xvalid, yvalid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = pd.read_csv(DATA_FOLDER+\"/train.csv\", index_col=\"row_id\")\n",
    "#test_df = pd.read_csv(DATA_FOLDER+\"/test.csv\", index_col=\"row_id\")\n",
    "#train_df.info()\n",
    "#test_df.info()\n",
    "# load the dataset\n",
    "cat_feats = ['date','country','store','product', 'num_sold']\n",
    "\n",
    "df = pd.read_csv(CFG.data_folder+\"/train.csv\", usecols = cat_feats)\n",
    "#print(df.head())\n",
    "for cc in cat_feats[1:]:\n",
    "    le = LabelEncoder()\n",
    "    df[cc] = le.fit_transform(df[cc])\n",
    "df = reduce_mem_usage(df)\n",
    "#print(df.head())\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "date = df[\"date\"].copy()\n",
    "df.set_index(\"date\",  inplace=True)\n",
    "df.num_sold.plot(xlabel = '')\n",
    "\n",
    "ytrain = df[\"num_sold\"].copy()\n",
    "\n",
    "xtrain = df.copy()\n",
    "xtrain.drop([\"num_sold\"], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain\n",
    "ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = MinMaxScaler()\n",
    "#scaler = RobustScaler()\n",
    "df = scaler.fit_transform(df)\n",
    "# use 50 historical observations, predict 10 step ahead\n",
    "look_back = 50\n",
    "look_ahead = 10\n",
    "\n",
    "\n",
    "xdat, ydat = create_dataset(df, look_back = look_back, look_ahead = look_ahead)\n",
    "\n",
    "# We only want to forecast a single value for each series => target is a column\n",
    "print(xdat.shape, ydat.shape)\n",
    "xtrain, ytrain, xvalid, yvalid = prepare_split(xdat, ydat, cutoff = 5000, timesteps = look_back)\n",
    "\n",
    "print(xtrain.shape, xvalid.shape, ytrain.shape, yvalid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ydat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df)\n",
    "df_scaled_robust = RobustScaler().fit_transform(df)\n",
    "df_scaled_minmax = MinMaxScaler().fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled_robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtrain.shape, ytrain.shape)\n",
    "look_back = 50\n",
    "look_ahead = 50\n",
    "xtrain = MinMaxScaler().fit_transform(df)\n",
    "reshape_ratio = xtrain.shape[0]//xtrain.shape[-1]\n",
    "xdat, ydat = create_dataset(xtrain, look_back = look_back, look_ahead = look_ahead)\n",
    "# reshape to [batch size, time steps, dimensionality]\n",
    "xtrain, ytrain, xvalid, yvalid = prepare_split(xdat, ydat, cutoff = 5000, timesteps= look_back)\n",
    "\n",
    "print(xtrain.shape, ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model1():    \n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(SimpleRNN(10,input_shape= [None,1]))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_model2(out_dim):    \n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(SimpleRNN(10,input_shape= [None,1], return_sequences = True))\n",
    "    model.add(SimpleRNN(10,input_shape= [None,1]))\n",
    "    model.add(Dense(out_dim))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "    return model\n",
    "    \n",
    "def create_model5(out_dim):    \n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(LSTM(10,input_shape= [None,1], return_sequences = True))\n",
    "    model.add(LSTM(10,input_shape= [None,1]))\n",
    "    model.add(Dense(out_dim))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error',optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def create_model_bigger(input):\n",
    "\n",
    "    model = Sequential([\n",
    "            #Input(shape = input.shape[-2:]),\n",
    "            Input(shape = [None,1]),\n",
    "            Bidirectional(LSTM(400, return_sequences = True)),\n",
    "            Bidirectional(LSTM(300, return_sequences = True)),\n",
    "            Bidirectional(LSTM(200, return_sequences = True)),\n",
    "            Bidirectional(LSTM(100, return_sequences = True)),\n",
    "            Dense(50, activation = 'selu'),\n",
    "            Dense(1),\n",
    "        ])\n",
    "    \n",
    "    model.compile(optimizer = \"adam\", loss = \"mae\")\n",
    "    \n",
    "    return model\n",
    "model1 = create_model1()\n",
    "#model2 = create_model2(look_ahead)\n",
    "#model5 = create_model5(look_ahead)\n",
    "bigger_model = create_model_bigger(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.summary()\n",
    "#model2.summary()\n",
    "#model5.summary()\n",
    "bigger_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "early_stop = EarlyStopping(monitor = 'val_loss', min_delta = 0.001, \n",
    "                           patience = 5, mode = 'min', verbose = 1,\n",
    "                           restore_best_weights = True)\n",
    "\n",
    "model5.fit(xtrain, ytrain, validation_data=(xvalid, yvalid), \n",
    "                  epochs = CFG.nepochs, batch_size = CFG.bsize, callbacks=[early_stop])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(ytrain)\n",
    "np.concatenate([xtrain, ytrain], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gkf = GroupKFold(n_splits = 3)\n",
    "#print((xtrain.shape, ytrain.shape))\n",
    "#print(xtrain[0])\n",
    "#print(enumerate(gkf.split(xtrain, ytrain, groups = date.unique())))\n",
    "#X = xtrain.reshape(xtrain.shape[1:])\n",
    "#print((xtrain.shape, ytrain.shape))\n",
    "#for i in enumerate(gkf.split(xtrain[:2], ytrain, groups = date.unique())):\n",
    "    #print(i)\n",
    "#for (fold, (id0, id1)) in enumerate(gkf.split(xtrain, ytrain, groups = date.unique())):\n",
    "    #print((fold, id0, id1))   \n",
    "    # \n",
    "look_back = 50\n",
    "look_ahead = 10\n",
    "\n",
    "\n",
    "xdat, ydat = create_dataset(xtrain, look_back = look_back, look_ahead = look_ahead)\n",
    "\n",
    "# We only want to forecast a single value for each series => target is a column\n",
    "print(xdat.shape, ydat.shape)\n",
    "xtrain, ytrain, xvalid, yvalid = prepare_split(xdat, ydat, cutoff = 5000, timesteps = look_back) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "type(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#gkf = GroupKFold(n_splits = 5)\n",
    "\n",
    "#print(gkf.get_n_splits(xtrain, ytrain, date))\n",
    "scheduler = ExponentialDecay(1e-3, 400*((len(xtrain)*0.8)/CFG.BATCH_SIZE), 1e-5)\n",
    "lr = LearningRateScheduler(scheduler, verbose = 1)\n",
    "        \n",
    "early_stop = EarlyStopping(monitor = 'val_loss', min_delta = 0.001, \n",
    "                        patience = 5, mode = 'min', verbose = 1,\n",
    "                        restore_best_weights = True\n",
    "                        )\n",
    "# fit \n",
    "bigger_model.fit(xtrain, ytrain, validation_data=(xvalid, yvalid), epochs = CFG.nepochs, batch_size = CFG.BATCH_SIZE, callbacks = [early_stop , lr])\n",
    "\n",
    "\"\"\"\n",
    "for (fold, (id0, id1)) in enumerate(gkf.split(xtrain, ytrain, groups = date.unique())):\n",
    "    print((fold, id0, id1))\n",
    "\n",
    "    X_train, X_valid = xtrain[id0], xtrain[id1]\n",
    "    y_train, y_valid = ytrain[id0], ytrain[id1]\n",
    "    model = create_model_bigger()        \n",
    "\n",
    "    scheduler = ExponentialDecay(1e-3, 400*((len(X_train)*0.8)/CFG.BATCH_SIZE), 1e-5)\n",
    "    lr = LearningRateScheduler(scheduler, verbose = 1)\n",
    "        \n",
    "    early_stop = EarlyStopping(monitor = 'val_loss', min_delta = 0.001, \n",
    "                            patience = 5, mode = 'min', verbose = 1,\n",
    "                            restore_best_weights = True\n",
    "                            )\n",
    "\n",
    "    # fit \n",
    "    model.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs = CFG.nepochs, batch_size = CFG.BATCH_SIZE, callbacks = [early_stop , lr])\n",
    "    break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict and calculate RMSE\n",
    "y_pred1 = model5.predict(xvalid)\n",
    "y_pred1 = scaler.inverse_transform(y_pred1)\n",
    "yvalid1 =  scaler.inverse_transform(yvalid)\n",
    "print('RMSE: ' + str(my_rmse(y_pred1, yvalid1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yvalid1, label = 'real')\n",
    "plt.plot(y_pred1, label = 'predicted')\n",
    "plt.ylabel('')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('tabular-playground-series-sep-2022-k3IcPUq4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0addc212d939e24309ea91cbb54b12c6d004085311e4cee26a07d84706d2303"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
